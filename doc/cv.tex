%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Medium Length Professional CV
% LaTeX Template
% Version 2.0 (8/5/13)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Trey Hunner (http://www.treyhunner.com/)
%
% Important note:
% This template requires the resume.cls file to be in the same directory as the
% .tex file. The resume.cls file provides the resume style used for structuring the
% document.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass{cv} % Use the custom resume.cls style

\usepackage[left=0.75in,top=0.6in,right=0.75in,bottom=0.6in]{geometry} % Document margins
\usepackage{hyperref}
\newcommand{\tab}[1]{\hspace{.2667\textwidth}\rlap{#1}}
\newcommand{\itab}[1]{\hspace{0em}\rlap{#1}}
\name{Clayton H. Sanford} % Your name
%\address{19 Clementina St. \#204 \\ San Francisco, CA 94105} % Your address
\address{\href{http://claytonsanford.com}{claytonsanford.com} \\ clayton@cs.columbia.edu \\ (831) 332-0431} % Your phone number and email
\address{LinkedIn: in/claytonsanford \\ Github: chsanford}

\begin{document}

%----------------------------------------------------------------------------------------
%	EDUCATION SECTION
%----------------------------------------------------------------------------------------

\begin{rSection}{Education}

\begin{rSubsection}{Columbia University}{September 2019 - May 2024 (expected)}{Ph.D. Candidate in Computer Science \\ M.S. (Feb. 2021), M.Phil. (Feb 2023)}{New York, NY}
\item Proposed Thesis: ``Neural Network Generalization and Approximation with Intrinsically Low-dimensional Data.''
\item Advisors: Rocco Servedio and Daniel Hsu
\end{rSubsection}

\begin{rSubsection}{Brown University}{September 2014 - May 2018}{Sc.B. with Honors in Applied Mathematics - Computer Science}{Providence, RI}
\item Thesis: ``Applying Rademacher-Like Bounds to Combinatorial Samples and Function Selection.''
\item Thesis Advisor: Eli Upfal; Concentration Advisor: Caroline Klivans
\item Magna Cum Laude
\end{rSubsection}

\end{rSection}


%----------------------------------------------------------------------------------------
%	WORK EXPERIENCE SECTION
%----------------------------------------------------------------------------------------

% \begin{rSection}{Research Experience}

% \begin{rSubsection}{Rademacher-Like Generalization Bounds}{November 2017 - present}{Bigdata Group}{Brown University Department of Computer Science}
% \item Researched under Professor Eli Upfal on extending uniform convergence bounds to new applications.
% \item Proved claims about the novel Cartesian EMD framework and wrote up findings in an honors thesis.
% \item Applied sample complexity techniques to audio denoising and compression algorithms.
% \item Architected experimental framework for testing the effectiveness of these bounds.
% \end{rSubsection}

% % \begin{rSubsection}{Learning Across Distributions}{June 2017 - May 2018}{Reinforcement Learning Group}{Brown University Department of Computer Science}
% % \item Researched under Professor Michael Littman on using samples drawn from different distributions to select a robust classifier that performs across combinations of those distributions.
% % \item Proved theoretical bounds on effectiveness of new cross-distribution algorithm.
% % \item Implemented algorithm and tested on reinforcement learning games.
% % \end{rSubsection}

% \begin{rSubsection}{Equation-Free Modeling of Traffic Systems}{September 2016 - February 2017}{Applied Dynamical Systems Group}{Brown University Division of Applied Mathematics}
% \item Researched with Bj\"orn Sandstede on modeling high-dimensional traffic models in low-dimensional spaces.
% \item Defined lifting and restriction operators to map low-dimensional instances to high-dimensional systems and vice-versa.
% \item Implemented equation-free modeling algorithms in Matlab and conducted simulations.
% \end{rSubsection}

% \begin{rSubsection}{Hassenfeld Child Health Innovation Institute Summer Scholar}{June 2016 - August 2016}{Fairbrother Lab}{Brown University Department of Molecular Biology}
% \item Awarded grant from Hassenfeld Child Health Innovation Institute to conduct scientific research related to child health under supervision of Professor William Fairbrother.
% \item Build the web framework of Spliceman 2, a tool that assesses the likelihood of mutations affecting RNA splicing.
% \end{rSubsection}

% \end{rSection}


%----------------------------------------------------------------------------------------
%	INDUSTRY EXPERIENCE SECTION
%----------------------------------------------------------------------------------------

\begin{rSection}{Industry Experience}

\begin{rSubsection}{Student Research (Incoming)}{January 2024 - March 2024}{Google Research}{New York, NY}
\item Will study fundamental capabilities and limitations of attention-based neural networks.
\end{rSubsection}

\begin{rSubsection}{Applied Sciences Intern}{May 2023 - August 2023}{Microsoft Research}{New York, NY}
\item Trained transformer models with up to 500 million parameters to learn combinatorial search tasks with behavioral cloning and chain-of-thought reasoning.
\item Proved theoretical results about the advantages of transformers over graph neural networks (GNNs) for identifying isomorphisms between different combinatorial problems, to support positive empirical results. (Manuscript in progress.) 
\end{rSubsection}

\begin{rSubsection}{Research Intern (PhD)}{May 2022 - August 2022}{Allen Institute for AI}{Seattle, WA}
\item Improved year-long temperature and humidity predictions of ML-corrected coarse-grid climate models by using novelty detection techniques.
\item Work presented at NeurIPS 2022 climate ML workshop and American Meteorological Society conference, and under review of the \textit{Journal of Advances in Modeling Earth Systems}.
\item Recognized with the Outstanding Internship award, a cash prize awarded to four AI2 interns.
\end{rSubsection}

\begin{rSubsection}{Software Engineering Intern}{April 2019 - August 2019}{Lumi Labs}{Palo Alto, CA}
\item Designed and built front-end (Objective C) and back-end (Java and Scala) features as at 15-person startup.
\item Implemented clustering algorithms on geographic data in Java.
\end{rSubsection}

\begin{rSubsection}{Associate Data Scientist}{August 2018 - April 2019}{LinkedIn}{San Francisco, CA}
\item Used Hive and SQL to create stable and frequently-used datasets that repopulate daily. 
\item Performed deep-dive analyses on open questions for the LinkedIn Learning product.
\item Co-coordinated a bi-weekly machine learning reading group.
\end{rSubsection}

\begin{rSubsection}{Data Analytics Intern}{June 2017 - August 2017}{LinkedIn}{San Francisco, CA}
\item Analyzed subscription patterns with LinkedIn Learning team using Pig, HDFS, SQL, and Python.
\item Contextualized findings in the Learning business and presented to stakeholders.
\end{rSubsection}

\end{rSection}


%----------------------------------------------------------------------------------------
%	PUBLICATIONS
%----------------------------------------------------------------------------------------
\begin{rSection}{Publications}
\begin{rSubsection}{Machine learning theory}{}{}{}{}
\item C. Sanford, D. Hsu, M. Telgarsky. \href{https://arxiv.org/abs/2306.02896}{``Representational strengths and limitations of transformers.''} \textit{Appearing at Neural Informational Processing Systems (NeurIPS) 2023.}

\item N. Ardeshir*, D Hsu*, C. Sanford*. \href{https://proceedings.mlr.press/v195/ardeshir23a.html}{``Intrinsic dimensionality and generalization properties of the R-norm inductive bias.''} \textit{Conference on Learning Theory (COLT) 2023.}

\item A. Bietta*, J. Bruna*, C. Sanford*, M. Song*. \href{https://proceedings.neurips.cc/paper_files/paper/2022/hash/3fb6c52aeb11e09053c16eabee74dd7b-Abstract-Conference.html}{``Learning single-index models with shallow neural networks.''} \textit{NeurIPS 2022.}

\item V. Chatziafratis*, I. Panageas*, C. Sanford*, S. Stavroulakis*. \href{https://proceedings.neurips.cc/paper_files/paper/2022/hash/755acd0c7c07180d78959b6d89768207-Abstract-Conference.html}{``On scrambling phenomena for randomly initialized recurrent networks.''} \textit{NeurIPS 2022.}

\item D. Hsu*, C. Sanford*, R. Servedio*, E.-V. Vlatakis-Gkaragkounis*. \href{https://proceedings.mlr.press/v178/hsu22a.html}{``Near-Optimal Statistical Query Lower Bounds for Agnostically Learning Intersections of Halfspaces with Gaussian Marginals.''} \textit{COLT 2022.}

\item C. Sanford, V. Chatziafratis. \href{https://proceedings.mlr.press/v151/sanford22a.html}{``Expressivity of Neural Networks via Chaotic Itineraries beyond Sharkovsky's Theorem.''} \textit{AISTATS 2022.}

\item N. Ardeshir*, C. Sanford*, D. Hsu. \href{https://proceedings.neurips.cc/paper/2021/hash/26d4b4313a7e5828856bc0791fca39a2-Abstract.html}{``Support vector machines and linear regression coincide with very high-dimensional features.''} \textit{NeurIPS 2021.}

\item D. Hsu*, C. Sanford*, R. Servedio*, E.-V. Vlatakis-Gkaragkounis*. \href{https://proceedings.mlr.press/v134/hsu21a.html}{``On the Approximation Power of Two-Layer Networks of Random ReLUs.''} \textit{COLT 2021.}
\end{rSubsection}

\begin{rSubsection}{Interdisciplinary ML and data science}{}{}{}{}
\item C. Sanford, A. Kwa, O. Watt-Meyer, S. Clark, N. Brenowitz, J. McGibbon, C. Bretherton. \href{https://essopenarchive.org/doi/full/10.22541/essoar.168500343.32924398}{``Improving the predictions of ML-corrected climate models with novelty detection.''} \textit{Appearing in Journal of Advances in Modeling Earth Systems.}

\item T. Chin*, J. Ruth*, C. Sanford*, R. Santorella*, P. Carter, B. Sandstede. \href{https://link.springer.com/article/10.1007/s10884-021-10127-w}{``Enabling equation-free modeling via diffusion maps.''} \textit{Journal of Dynamics and Differential Equations,} 2022.

\item K. Cygan*, C. Sanford*, W. Fairbrother. \href{https://academic.oup.com/bioinformatics/article/33/18/2943/3887237?login=true}{``Spliceman2 - A Computational Web Server That Predicts Sequence Variations in Pre-mRNA Splicing.''}
\textit{Bioinformatics} 33 (18), 2017.
\end{rSubsection}
%C. Sanford, D. Hsu, M. Telgarsky. ``Representational strengths and limitations of transformers.'' \textit{Neural Informational Processing Systems (NeurIPS) 2023.}
%
%C. Sanford, A. Kwa, O. Watt-Meyer, S. Clark, N. Brenowitz, J. McGibbon, C. Bretherton. ``Improving the predictions of ML-corrected climate models with novelty detection.'' \textit{Appearing in Journal of Advances in Modeling Earth Systems (JAMES).}
%
%C. Sanford*, N. Ardeshir*, D. Hsu. ``Intrinsic dimensionality and generalization properties of the R-norm inductive bias.'' \textit{Conference on Learning Theory (COLT)}, 2023.
%
%A. Bietti*, J. Bruna*, C. Sanford*, M. Song*. ``Learning single-index models with shallow neural networks.'' \textit{Neural Information Processing Systems (NeurIPS)}, 2022.
%
%V. Chatziafratis*, I. Panageas*, C. Sanford*, S. Stavroulakis*. ``On scrambling phenomena for randomly initialized recurrent networks.'' \textit{Neural Information Processing Systems (NeurIPS)}, 2022.
%
%D. Hsu*, C. Sanford*, R. Servedio*, E.-V. Vlatakis-Gkaragkounis*. ``Near-Optimal Statistical Query Lower Bounds for Agnostically Learning Intersections of Halfspaces with Gaussian Marginals.'' \textit{Conference on Learning Theory (COLT)}, 2022.
%
%C. Sanford, V. Chatziafratis. ``Expressivity of Neural Networks via Chaotic Itineraries beyond Sharkovsky's Theorem.'' \textit{AISTATS}, 2022.
%
%T. Chin*, J. Ruth*, C. Sanford*, R. Santorella*, P. Carter*, B. Sandstede*. ``Enabling equation-free modeling via diffusion maps.'' \textit{Journal of Dynamics and Differential Equations}, 2022.
%
%
%N. Ardeshir*, C. Sanford*, D. Hsu. ``Support vector machines and linear regression coincide with very high-dimensional features.'' \textit{Neural Information Processing Systems (NeurIPS)}, 2021.
%
%
%D. Hsu*, C. Sanford*, R. Servedio*, E.-V. Vlatakis-Gkaragkounis*. ``On the Approximation Power of Two-Layer Networks of Random ReLUs.'' \textit{Conference on Learning Theory (COLT)}, 2021.
%
%
%% C. Cousins, C. Sanford, E. Upfal. ``Welfare-Optimal Codec Selection with Uniform Convergence Bounds.'' \textit{Submitted for Publication}.
%
%% T. Chin*, J. Ruth*, C. Sanford*, R. Santorella*, P. Carter, B. Sandstede. ``Using Diffusion Maps in Equation-Free Modeling.'' \textit{Preprint}.
%
%K. Cygan*, C. Sanford*, W. Fairbrother. ``Spliceman2 - A Computational Web Server That Predicts Sequence Variations in Pre-mRNA Splicing.'' \\
%\textit{Bioinformatics} 33 (18), 2017.
%
%J. Gross*, C.Sanford*, G. Kocks*. ``Projected Water Needs and
%Intervention Strategies in India.''\\
%\textit{Undergraduate Mathematics and its Applications} 37 (2), 2016.
%
%* Contributed equally
\end{rSection}



%	AWARDS
%----------------------------------------------------------------------------------------

\begin{rSection}{Fellowships and Awards} \itemsep -2pt

\begin{rSubsection}{NSF GRFP Fellowship}{March 2021}{National Science Foundation}{}
\item Competitive fellowship that provides three years of full funding for graduate research.
\end{rSubsection}

\begin{rSubsection}{Paul Charles Michelman Memorial Award}{May 2023}{Columbia Computer Science}{}
\item Given to a PhD student in Computer Science who has performed exemplary service to the department, devoting time and effort beyond the call to further the department’s goals (cash prize).
\end{rSubsection}

\begin{rSubsection}{Department Service Award}{May 2020, 2022, 2023}{Columbia Computer Science}{}
\item Awarded up to 10\% of PhD students for their service to the department.
\end{rSubsection}

\begin{rSubsection}{Outstanding Intern Award}{December 2022}{Allen Institute for AI}{}
\item Awarded to four summer interns who went above and beyond as researchers and as colleagues (cash prize).
\end{rSubsection}

\begin{rSubsection}{Computer Science Senior Prize}{May 2018}{Brown Computer Science}{}
\item Awarded to the top students in the computer science department based on academic achievement and department service (cash prize).
\end{rSubsection}

\begin{rSubsection}{Outstanding Winner}{April 2016}{Interdisciplinary Contest in Modeling}{Consortium for Mathematics and its Applications}
\item Designation given to five out of over 3000 teams for mathematical modeling of water scarcity in the ICM contest.
% \item Paper published in the UMAP journal as a result.
\end{rSubsection}

\end{rSection}

\begin{rSection}{Leadership and Mentorship Experience}

\begin{rSubsection}{Community Board Member}{May 2023 - present}{Manhattan Community Board 9}{New York, New York}
\item Appointed by the borough president to represent community needs of a district on the west side of Manhattan between 110th and 155th St. 
\item Serves on the the Economic Development/West Harlem Piers Committee and the LGBTQ Task Force.
\end{rSubsection}


\begin{rSubsection}{PhD Representative}{May 2022 - present}{Department of Computer Science}{Columbia University}
\item Coordinated a well-attended PhD student welcome event to help new students visit.
\item Attends faculty meetings to represent student concerns and communicate faculty decisions to student body.
\item Personally assisted students ensure that the department is paying them adequately and assisted international students with CPT approval issues.
\end{rSubsection}

\begin{rSubsection}{President}{September 2022 - September 2023}{qSTEM (LGBTQ affinity for School of Engineering)}{Columbia University}
\item Organized coffee hours, happy hours, movie showings, and board game nights to build community among LGBTQ grad students.
\end{rSubsection}


\begin{rSubsection}{President}{February 2015 - May 2018}{Applied Math Department of Undergraduates (APMA DUG)}{Brown University}
\item Hosted well-attended advising panels for students interested in Applied Math courses and research.
\item Created problems for and managed a casual math competition every semester.
\item Coordinated lectures by Applied Math faculty members for undergrads every semester.
\item Welcomed prospective students and new concentrators by planning department-sponsored celebrations.
\end{rSubsection}

\begin{rSubsection}{President}{November 2014 - May 2018}{Outing Club}{Brown University}
\item Led an executive board of forty members that ran trips every weekend of the academic year.
\item Managed and apportioned a \$27000 annual budget.
\item Recruited, interviewed, and trained new trip leaders.
\end{rSubsection}

\begin{rSubsection}{Peer Advisor}{September 2017 - May 2018}{Matched Advising Program for Sophomores (MAPS)}{Brown University}
\item Advised two sophomore Applied Math students as they declared their concentrations and decided on coursework and internships.
\end{rSubsection}

\begin{rSubsection}{Peer Advisor}{September 2015 - May 2017}{Meiklejohn Peer Advisory Program}{Brown University}
\item Advised eleven first year students on adjusting to college life, selecting courses, building connections, and finding their academic paths.
\end{rSubsection}

\end{rSection}


%----------------------------------------------------------------------------------------
\begin{rSection}{Relevant Coursework}
\textbf{Algorithms and Theory:} Models of Computation, Analysis and Design of Algorithms, Advanced Algorithms Seminar, Computational Linear Algebra, Intro to Cryptography, Randomized Algorithms, Computation and the Brain

\textbf{Artificial Intelligence:} Machine Learning, Artificial Intelligence, Foundations of Prescriptive Analytics, Independent Study for ML research, Optimization Methods for ML, ML Theory, Algorithmic Game Theory

\textbf{Probability and Statistics:} Probability and Computation, Information Theory, Recent Applications in Probability and Statistics, Probabilistic Methods in Computer Science

\textbf{Dynamical Systems:} Applied Ordinary Differential Equations, Applied Partial Differential Equations I, Topics in Chaotic Dynamics, Independent Study for Dynamical Systems Research

\textbf{Pure Mathematics:} Linear Algebra, Abstract Algebra, Analysis: Functions of One Variable

\textbf{Non-Technical:} Persuasive Communication, Classrooms in Context: Public Education in Providence

\end{rSection}


%----------------------------------------------------------------------------------------
%	TALKS
%----------------------------------------------------------------------------------------

%%%%%% ERIC: Consider removing

% \begin{rSection}{Talks Given} \itemsep -2pt

% \begin{rSubsection}{Theories in Action}{May 2018}{``Applying Scientific Research''}{Brown University Curricular Resource Center}
% \item Presented honors thesis and motivated the need for confidence in ML algorithms on a panel to an audience without no expected background in computer science.
% \end{rSubsection}

% \begin{rSubsection}{Honors Thesis Defense}{April 2018}{ }{Brown University Department of Computer Science}
% \item Successfully defended my thesis in a twenty-minute talk with ten minutes of questions from faculty.
% \end{rSubsection}

% \begin{rSubsection}{High School Computer Science Class Presentation}{January 2018}{``Machine Learning and Artificial Intelligence''}{Soquel High School}
% \item Introduced high school students to machine learning basics and ethical questions of artificial intelligence in a forty-minute lecture given to two high school computer science classes.
% \end{rSubsection}

% \begin{rSubsection}{Math Slam}{March 2017}{``Equation-Free Modeling''}{Brown University Society for Industrial and Applied Mathematics}
% \item Ten-minute research talk about independent study with Bj\"orn Sandstede on equation-free modeling of high-dimensional dynamical systems.
% \end{rSubsection}

% \end{rSection}

%----------------------------------------------------------------------------------------
%	TEACHING EXPERIENCE SECTION
%----------------------------------------------------------------------------------------

\begin{rSection}{Teaching Experience}
\begin{rSubsection}{Graduate Teaching Assistant}{September 2023 - December 2023}{Columbia University Department of Computer Science}{}
\item Provided readings for course syllabus, graded assignments, and mentored student final projects for the Machine Learning and Climate graduate seminar, taught by Alp Kucukelbir.
\end{rSubsection}

\begin{rSubsection}{Graduate Instructor}{January 2022 - May 2022}{Columbia University Department of Computer Science}{}
\item Developed and taught a lab on basics of data science and ML for non-CS students to accompany a then-new class on Natural and Artificial Neural Networks by Christos Papaidimitriou.
\item Created a series of Colab notebooks and short lectures to accompany each topic for a lab with fifteen students.
\end{rSubsection}


\begin{rSubsection}{Graduate Teaching Assistant}{January 2021 - April 2021}{Columbia University Department of Computer Science}{}
\item Held office hours, graded assignments, and prepared course materials for Introduction to Computational Learning Theory, taught by Rocco Servedio.
\end{rSubsection}

\begin{rSubsection}{Head Teaching Assistant}{April 2017 - December 2017}{Brown University Department of Computer Science}{}
\item Led a staff of 14 UTAs through grading assignments, running review sessions, and holding office hours.
\item Hired UTAs after interviewing 35 candidates for the job.
\item Managed an Algorithms class with 170 students and coordinated interactive grading sessions and exams.
\item Taught an supplemental section on NP-hardness to a group of forty students for 90 minutes.
\item Brainstormed, wrote-up, and edited problems for homework assignments and exams. 
\end{rSubsection}

\begin{rSubsection}{Undergraduate Teaching Assistant}{September 2015 - May 2017}{Brown University Departments of CS and Applied Math}{}
\item Served on the course staffs of four courses: Accelerated Intro to CS, Discrete Structures and Probability, Theory of Computation, Topics in Chaotic Dynamics.
\item Created problems for and graded homework assignments and exams.
\item Hosted office hours for helping students understand course material and solve homework problems.
\end{rSubsection}

\begin{rSubsection}{Tutor and Volunteer Representative}{January 2015 - May 2016}{Swearer Tutoring Enrichment in Math and Science (STEMS)}{}
\item Tutored math and science in class and after school at a nearby public school in Providence.
\item Interviewed potential volunteers and planned meetings to help train tutors.
\end{rSubsection}

%%%%% ERIC: consider removing
\begin{rSubsection}{Tutor}{September 2011 - June 2014}{Soquel High School}{}
\item Tutored math at homework club after school twice a week for three years.
\end{rSubsection}

\end{rSection}

%----------------------------------------------------------------------------------------
%	POSITIONS OF RESPONSIBILITY
%----------------------------------------------------------------------------------------

\begin{rSection}{Research Talks}
\begin{rSubsection}{}{}{}

\item Google Research Mountain View talk, January 2024. ``Representational Strengths and Limitations of Transformers.''
\item Formal Languages and Neural Networks (FLANN) talk, virtual, November 2023. ``Representational Strengths and Limitations of Transformers.''
\item Joan Bruna group meeting, NYU, October 2023. ``Representational Strengths and Limitations of Transformers.''
\item ``Theory and Practice of Foundation Models'' invited speaker, Yale, October 2023. ``Representational Strengths and Limitations of Transformers.''
\item Algorithms seminar, Google NYC, July 2023. ``Representational Strengths and Limitations of Transformers.''
\item Data Science guest lecture, UC San Diego, May 2023. ``Representational Strengths and Limitations of Transformers.''
\item Engaged Scholars Program Research Symposium, Columbia University, April 2023. ``Machine Learning, Neural Networks, and CS Theory.''
\item Columbia StatisticalML Symposium, Columbia University, April 2023. ``Transformers can learn pairwise---but not three-wise---functions.''
\item 22nd Conference on Artificial Intelligence for Environmental Science, 103rd American Meteorological Society Annual Meeting, January 2023. ``Improving the Predictions of ML-Corrected Climate Models with Novelty Detection.''
\item Tackling Climate Change with Machine Learning workshop, NeurIPS 2022, December 2022. ``Improving the Predictions of ML-Corrected Climate Models with Novelty Detection.''
\item Vaggos Chatziafratis group meeting, UC Santa Cruz, November 2022. ``Why do over-parameterized neural networks work?''
\item Kevin Jamieson, Jamie Morgenstern, and Ludwig Schmidt group meeting, University of Washington, July 2022. ``Approximation Powers and Limitations of Neural Networks.''
\item COLT 2022, July 2022.  ``Near-Optimal Statistical Query Lower Bounds for Agnostically Learning Intersections of Halfspaces with Gaussian Marginals.''
\item Algorithms and Theory Seminar, Boston University, April 2022. ``On the approximation power of two-layer networks of random ReLUs.'' 
\item Algorithms and Complexity seminar, MIT, April 2022. ``On the approximation power of two-layer networks of random ReLUs.'' 
\item Eli Upfal group meeting, Brown University, April 2022. ``“Benign overfitting” and the behavior of high-dimensional linear regression and classification models.''
\item AISTATS, March 2022. ``Expressivity of Neural Networks via Chaotic Itineraries beyond Sharkovsky's Theorem.'' 
\item  Joan Bruna group meeting, NYU, February 2022. ``Near-Optimal Statistical Query Lower Bounds for Agnostically Learning Intersections of Halfspaces with Gaussian Marginals.''
\item  Data Science Institute virtual poster session Columbia, February 2022. ``On the approximation power of two-layer networks of random ReLUs.''
\item  NeurIPS, December 2021. ``Support vector machines and linear regression coincide with very high-dimensional features.''
\item COLT 2021, August 2021.  ``On the approximation power of two-layer networks of random ReLUs.''
\item Demystifying the dissertation, Columbia, December 2020. ``Opening the Black Box: Mathematical Approaches to Understanding Deep Learning.''
\item Demystifying the PhD, Columbia, November 2020. 

\end{rSubsection}
\end{rSection}

\begin{rSection}{Department Service}
\begin{rSubsection}{}{}{}

% \begin{itemize}
    \item Organized the CS theory student retreat in Fall 2021 and 2022.
    \item Ran events at and coordinated the Columbia Visit Day for admitted students in Spring 2020, 2022, and 2023.
    \item Started the Columbia Theory Student seminar, where students share their research on a weekly basis.
    \item Advised three cohorts of undergraduate theory seminars on ML and deep learning theory.
% \end{itemize}
\end{rSubsection}
\end{rSection}

\begin{rSection}{Academic Service}
\begin{rSubsection}{Reviewer}{}{}

% \begin{itemize}
    \item ICLR (2024), ALT (2024), JMLR (2023), NeurIPS (2023 (top reviewer)), ICLR climate workshop (2023), SODA (2022), STOC (2022).
% \end{itemize}
\end{rSubsection}

\end{rSection}

 %----------------------------------------------------------------------------------------
%	TECHNICAL SKILLS SECTION
%----------------------------------------------------------------------------------------

\begin{rSection}{Technical skills and interests}

\begin{tabular}{ @{} >{\bfseries}l @{\hspace{6ex}} l }
Programming Languages &  Python, Java, Scala, SQL \\
Technologies & Pytorch, Tensorflow, Docker, Hadoop, Spark, Git  \\
Spoken Languages & English (native), Spanish (intermediate proficiency) \\
Other Interests & Backpacking, Running, Climbing, Cooking, New York, Public Transit
\end{tabular}

\end{rSection}


\end{document}
